\section*{2. Convex Functions}
\subsection*{Cauchy-Schwarz Inequality}
\begin{enumerate}[label = (\arabic*), leftmargin=*]
    \item $|\langle\vect{u}, \mathbf{v}\rangle|^{2} \leq\langle\mathbf{u}, \mathbf{u}\rangle \cdot\langle\mathbf{v}, \mathbf{v}\rangle$
    \item $\x^\top \y \leq \|\x\|_2 \cdot \|\y\|_2$
    \item $\x^\top \y \leq \|\x\|_{\bullet} \|\y\|_*$ in general.
    \item $-1 \leq \frac{\mathbf{u}^{\top} \mathbf{v}}{\|\mathbf{u}\|\|\mathbf{v}\|} \leq 1$
    \item Triangle Ineq: $\|\mathbf{u}+\mathbf{v}\| \leq\|\mathbf{u}\|+\|\mathbf{v}\|$
    \item $\|\mathbf{a}+\mathbf{b}\|_{2}^{2} \leqslant 2\|\mathbf{a}\|_{2}^{2}+2\|\mathbf{b}\|_{2}^{2}$
    \item Cosine: $\cos \theta_{\mathbf{u v}}=\frac{\langle\mathbf{u}, \mathbf{v}\rangle}{\|\mathbf{u}\|\|\mathbf{v}\|}$
    \item Var, Cov: $\operatorname{Cov}(X, Y)^{2} \leqslant \operatorname{Var}(Y) \operatorname{Var}(X)$
    \item Expectation: $\E (X Y)^{2} \leqslant \E\left(X^{2}\right) \E \left(Y^{2}\right)$
\end{enumerate}



\subsection*{Mean Value Theorem}
Let $a<b$, $a, b \in \mathbb{R}$, and $h:[a, b] \rightarrow \mathbb{R}$ be a continuous func that is differentiable on $(a, b)$. Then there exists $c \in(a, b)$ s.t.
$$
h^{\prime}(c)=\frac{h(b)-h(a)}{b-a}
$$

\subsection*{Convex Set}
A set $C \subseteq \mathbb{R}^{d}$ is convex if the line segment between any two points of $C$ lies in $C$, i.e., if for any $\mathbf{x}, \mathbf{y} \in C$ and any $\lambda$ with $0 \leq \lambda \leq 1$, we have
$$
\lambda \mathbf{x}+(1-\lambda) \mathbf{y} \in C .
$$
\subsubsection*{Intersections of convex sets are convex.}

\subsection*{Convex Functions}
A function $f: \operatorname{dom}(f) \rightarrow \mathbb{R}$ is convex if (i) $\operatorname{dom}(f)$ is a convex set and (ii) for all $\mathbf{x}, \mathbf{y} \in \operatorname{dom}(f)$, and $\lambda$ with $0 \leq \lambda \leq 1$, we have
$$
f(\lambda \mathbf{x}+(1-\lambda) \mathbf{y}) \leq \lambda f(\mathbf{x})+(1-\lambda) f(\mathbf{y})
$$

\begin{itemize}[leftmargin=*]
    \item The \green{graph} of a function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ is defined as
        $$
        \{(\mathbf{x}, f(\mathbf{x})) \mid \mathbf{x} \in \operatorname{dom}(f)\}
        $$
    \item The \green{epigraph} of a function $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ is defined as
$$
\operatorname{epi}(f):=\left\{(\mathbf{x}, \alpha) \in \mathbb{R}^{d+1} \mid \mathbf{x} \in \operatorname{dom}(f), \alpha \geq f(\mathbf{x})\right\},
$$
    \item $f$ is a convex function if and only if $\operatorname{epi}(f)$ is a convex set.
\end{itemize}

\subsubsection*{Examples of convex functions} 
\begin{itemize}[leftmargin=*]
    \item Affine, Square, Exponential
    \item Norm: Every norm is convex.
\end{itemize}

\subsubsection*{Convex Functions are Continuous}
Let $f$ be convex and suppose that $\operatorname{dom}(f) \subseteq \mathbb{R}^{d}$ is open. Then $f$ is continuous.

\subsection*{Jensen's Inequality}
Let $f$ be convex, $\mathbf{x}_{1}, \ldots, \mathbf{x}_{m} \in \mathbf{d o m}(f), \lambda_{1}, \ldots, \lambda_{m} \in \mathbb{R}_{+}$such that $\sum_{i=1}^{m} \lambda_{i}=1$. Then
$$
f\left(\sum_{i=1}^{m} \lambda_{i} \mathbf{x}_{i}\right) \leq \sum_{i=1}^{m} \lambda_{i} f\left(\mathbf{x}_{i}\right)
$$

\begin{itemize}[leftmargin=*]
    \item Expectation: If $X$ is a random variable and $\varphi$ is a convex function, then
    $$
    \varphi(\E[X]) \leq \E[\varphi(X)]
    $$
\end{itemize}

\subsection*{Differentiable Functions}
Let $f: \operatorname{dom}(f) \rightarrow \mathbb{R}^{m}$ where $\operatorname{dom}(f) \subseteq \mathbb{R}^{d}$ is open. $f$ is called differentiable at $\mathbf{x} \in \operatorname{dom}(f)$ if there exists an $(m \times d)$-matrix $\vect{A}$ and an error function $r: \mathbb{R}^{d} \rightarrow \mathbb{R}^{m}$ defined around $\mathbf{0} \in \mathbb{R}^{d}$ such that for all $\mathbf{y}$ in some neighborhood of $\mathbf{x}$,
$$
f(\mathbf{y})=f(\mathbf{x})+\vect{A}(\mathbf{y}-\mathbf{x})+r(\mathbf{y}-\mathbf{x})
$$
where
$$
\lim _{\mathbf{v} \rightarrow \mathbf{0}} \frac{\|r(\mathbf{v})\|}{\|\mathbf{v}\|}=\mathbf{0} . \quad \text { (Error } r \text { is sublinear) }
$$

\begin{itemize}[leftmargin=*]
    \item $\vect{A}$ is unique and called the \green{differential} or \green{Jacobian} matrix of $f$ at $\mathbf{x}$.
    \item Graph of the affine function $f(\mathbf{x})+\nabla f(\mathbf{x})^{\top}(\mathbf{y}-\mathbf{x})$ is a \green{tangent hyperplane} to the graph of $f$ at $(\mathbf{x}, f(\mathbf{x}))$.
\end{itemize}

\subsection*{Lemma 2.15 First-order Characterization of Convexity}
$f$ is convex if and only if \red{$\operatorname{dom}(f)$ is convex} and
$
f(\mathbf{y}) \geq f(\mathbf{x})+\nabla f(\mathbf{x})^{\top}(\mathbf{y}-\mathbf{x})
$
holds for all $\mathbf{x}, \mathbf{y} \in \mathbf{d o m}(f)$.

\subsection*{Lemma 2.17 Second-order Characterization of Convexity}
$f$ is convex if and only if \red{$\operatorname{dom}(f)$ is convex}, and for all $\mathbf{x} \in \operatorname{dom}(f)$, we have
$
\nabla^{2} f(\mathbf{x}) \succeq 0
$



\subsection*{Lemma 2.16 Monotonicity of the Gradient}
Suppose that $\operatorname{dom}(f)$ is open and that $f$ is differentiable. Then $f$ is convex \red{iff} $\operatorname{dom}(f)$ is convex and
$$
(\nabla f(\mathbf{y})-\nabla f(\mathbf{x}))^{\top}(\mathbf{y}-\mathbf{x}) \geq 0
$$
holds for all $\mathbf{x}, \mathbf{y} \in \operatorname{dom}(f)$.

The inequality in monotonicity of the gradient is strict unless $\x=\y$ or $f(\lambda \x+(1-\lambda) \y)=\lambda f(\x)+(1-\lambda) f(\y)$ for all $\lambda \in(0,1)$.


\subsection*{Lemma 2.18 Operations that Preserve Convexity}
\begin{enumerate}[label = (\arabic*), leftmargin=*]
    \item Let $f_{1}, f_{2}, \ldots, f_{m}$ be \red{convex} functions, $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m} \in \mathbb{R}_{+}$. Then
$
f:=\max _{i=1}^{m} f_{i}
$
as well as
$
f:=\sum_{i=1}^{m} \lambda_{i} f_{i}
$
are convex on $\operatorname{dom}(f):=\bigcap_{i=1}^{m} \operatorname{dom}\left(f_{i}\right)$.
   
    \item Let $f$ be a \red{convex} function with $\operatorname{dom}(f) \subseteq \mathbb{R}^{d}, g: \mathbb{R}^{m} \rightarrow \mathbb{R}^{d}$, $g(\mathbf{x})=A \mathbf{x}+\mathbf{b}$, for some matrix $\vect{A} \in \mathbb{R}^{d \times m}$ and vector $\mathbf{b} \in \mathbb{R}^{d}$. Then the function $f \circ g$ is convex on $\operatorname{dom}(f \circ g):=\left\{\mathbf{x} \in \mathbb{R}^{m}: g(\mathbf{x}) \in \operatorname{dom}(f)\right\}$.
\end{enumerate}

\subsection*{Local \& Global Minima}
A local minimum of $f: \operatorname{dom}(f) \rightarrow \mathbb{R}$ is a point $\mathbf{x}$ such that there exists $\varepsilon>0$ with
$$
f(\mathbf{x}) \leq f(\mathbf{y}) \quad \forall \mathbf{y} \in \mathbf{d o m}(f) \text { satisfying }\|\mathbf{y}-\mathbf{x}\|<\varepsilon
$$
Meaning: in some small neighborhood, $\mathbf{x}$ is the best point.

\subsubsection*{Lemma 2.20}
Let $\mathbf{x}^{\star}$ be a \green{local minimum} of a convex function $f: \operatorname{dom}(f) \rightarrow \mathbb{R}$. Then $\mathbf{x}^{\star}$ is a \green{global minimum}, meaning that $f\left(\mathbf{x}^{\star}\right) \leq f(\mathbf{y}) \quad \forall \mathbf{y} \in \operatorname{dom}(f)$.

\subsubsection*{Lemma 2.22}
Suppose that $f: \operatorname{dom}(f) \rightarrow \mathbb{R}$ is differentiable over an open domain $\operatorname{dom}(f) \subseteq \mathbb{R}^{d}$. Let $\mathbf{x} \in \operatorname{dom}(f)$. If $\mathbf{x}$ is a global minimum then $\nabla f(\mathbf{x})=\mathbf{0}$ (a \green{critical point}).

\subsubsection*{Lemma 2.21}
For convex func, the converse of Lemma 2.22 is also true: If $\nabla f(\mathbf{x})=\mathbf{0}$, then $\mathbf{x}$ is a global minimum.

\subsection*{Strictly Convex Functions}
A function $f: \operatorname{dom}(f) \rightarrow \mathbb{R}$ is \green{strictly convex} if (i) $\operatorname{dom}(f)$ is convex and (ii) for all $\mathbf{x} \neq \mathbf{y} \in \operatorname{dom}(f)$ and all $\lambda \in(0,1)$, we have
$$
f(\lambda \mathbf{x}+(1-\lambda) \mathbf{y})<\lambda f(\mathbf{x})+(1-\lambda) f(\mathbf{y})
$$

\subsubsection*{Lemma 2.25}
Strictly convex func has \red{at most one} global min.

\subsection*{Constrained Minimization}
A point $\mathbf{x} \in X$ is a \green{minimizer} of $f$ over $X$ if
$$
f(\mathbf{x}) \leq f(\mathbf{y}) \quad \forall \mathbf{y} \in X .
$$
\subsubsection*{Lemma 2.27 Optimality Condition}
Suppose that $f: \operatorname{dom}(f) \rightarrow \mathbb{R}$ is convex and differentiable over an open domain $\operatorname{dom}(f) \subseteq \mathbb{R}^{d}$, and let $X \subseteq \operatorname{dom}(f)$ be a convex set. Point $\mathbf{x}^{\star} \in X$ is a minimizer of $f$ over $X$ if and only if
$$
\nabla f\left(\mathbf{x}^{\star}\right)^{\top}\left(\mathbf{x}-\mathbf{x}^{\star}\right) \geq 0 \quad \forall \mathbf{x} \in X
$$

\subsubsection*{Sublevel}
$f: \mathbb{R}^{d} \rightarrow \mathbb{R}, \alpha \in \mathbb{R}$. The set $f^{\leq \alpha}:=\left\{\mathbf{x} \in \mathbb{R}^{d}: f(\mathbf{x}) \leq \alpha\right\}$ is the \green{$\alpha$-sublevel} set of $f$.

\subsubsection*{Thm 2.29 (Weierstrass Theorem)}
Let $f: \mathbb{R}^{d} \rightarrow \mathbb{R}$ be a convex function, and suppose there is a nonempty and bounded sublevel set $f^{\leq \alpha}$. Then $f$ has a global minimum.

\subsection*{Optimization Problem in Standard Forms}
$$
\begin{array}{ll}
\operatorname{minimize} & f_{0}(\mathbf{x}) \\
\text { subject to } & f_{i}(\mathbf{x}) \leq 0, \quad i=1, \ldots, m \\
& h_{i}(\mathbf{x})=0, \quad i=1, \ldots, p
\end{array}
$$

Domain
$
\mathcal{D}=\left\{\cap_{i=0}^{m} \operatorname{dom}\left(f_{i}\right)\right\} \cap \left\{ \cap_{i=1}^{p} \operatorname{dom}\left(h_{i}\right)\right\}
$

\green{Convex program}: All $f_{i}$ are convex functions, and all $h_{i}$ are affine functions with domain $\mathbb{R}^{d}$.

\subsection*{Lagrangian}
Given an optimization problem in standard form, its \green{Lagrangian} is the func $L: \mathcal{D} \times \mathbb{R}^{m} \times \mathbb{R}^{p} \rightarrow \mathbb{R}$ given by
$$
L(\mathbf{x}, \boldsymbol{\lambda}, \boldsymbol{\nu})=f_{0}(\mathbf{x})+\sum_{i=1}^{m} \lambda_{i} f_{i}(\mathbf{x})+\sum_{i=1}^{p} \nu_{i} h_{i}(\mathbf{x})
$$

The $\lambda_{i}, \nu_{i}$ are called \green{Lagrange multipliers}.
The \green{Lagrange dual function} is the function $g: \mathbb{R}^{m} \times \mathbb{R}^{p} \rightarrow \mathbb{R} \cup\{-\infty\}$ defined by
$$
g(\boldsymbol{\lambda}, \boldsymbol{\nu})=\inf _{\mathbf{x} \in \mathcal{D}} L(\mathbf{x}, \boldsymbol{\lambda}, \boldsymbol{\nu}) .
$$

\subsection*{Lemma 2.45 Weak Lagrange Duality}
Short Ver.: Lagrange dual function values are lower bounds on primal function values $f_{0}(\mathbf{x})$.

Long Ver.: Let $\mathbf{x}$ be a feasible solution, meaning that $f_{i}(\mathbf{x}) \leq 0$ for $i=1, \ldots, m$ and $h_{i}(\mathbf{x})=0$ for $i=1, \ldots, p$. Let $g$ be the Lagrange dual function of and $\boldsymbol{\lambda} \in \mathbb{R}^{m}, \boldsymbol{\nu} \in \mathbb{R}^{p}$ such that $\boldsymbol{\lambda} \geq \mathbf{0}$. Then
$$
g(\boldsymbol{\lambda}, \boldsymbol{\nu}) \leq f_{0}(\mathbf{x})
$$

Choose $\boldsymbol{\lambda} \geq \mathbf{0}$ and $\boldsymbol{\nu}$ such that $g(\boldsymbol{\lambda}, \boldsymbol{\nu})$ is maximized!

By weak duality, the \green{supremum} value of the Lagrange dual is a lower bound for the
\green{infimum} value of the primal problem.

\subsection*{Thm 2.47 Strong Lagrange Duality}
Suppose that a convex program has a feasible solution $\tilde{\mathbf{x}}$ that in addition satisfies $f_{i}(\tilde{\mathbf{x}})<0, i=1, \ldots, m$ (a \green{Slater point}). Then the infimum value of the primal equals the supremum value of its Lagrange dual. Moreover, if this value is finite, it is attained by a feasible solution of the dual.

Convex programming with Slater point and finite value: $\inf f_{0}(\mathbf{x})=\max g(\boldsymbol{\lambda}, \boldsymbol{\nu})$.

\subsection*{Zero Duality Gap}
Let $\tilde{\mathbf{x}}$ be feasible for the primal and $(\tilde{\boldsymbol{\lambda}}, \tilde{\boldsymbol{\nu}})$ feasible for the Lagrange dual. The primal and dual solutions $\tilde{\mathbf{x}}$ and $(\tilde{\boldsymbol{\lambda}}, \tilde{\boldsymbol{\nu}})$ are said to have \green{zero duality gap} if $f_{0}(\tilde{\mathbf{x}})=g(\tilde{\boldsymbol{\lambda}}, \tilde{\boldsymbol{\nu}})$.

\subsection*{Lemma 2.49 Complementary Slackness}
If $\tilde{\mathbf{x}}$ and $(\tilde{\boldsymbol{\lambda}}, \tilde{\boldsymbol{\nu}})$ have zero duality gap, then
$$
\tilde{\lambda}_{i} f_{i}(\tilde{\mathbf{x}})=0, \quad i=1, \ldots, m .
$$

\subsection*{Lemma 2.50 Vanishing Lagrangian Gradient}
If $\tilde{\mathbf{x}}$ and $(\boldsymbol{\lambda}, \tilde{\boldsymbol{\nu}})$ have zero duality gap, and if all $f_{i}$ and $h_{i}$ are differentiable, then
$$
\nabla f_{0}(\tilde{\mathbf{x}})+\sum_{i=1}^{m} \tilde{\lambda}_{i} \nabla f_{i}(\tilde{\mathbf{x}})+\sum_{i=1}^{p} \tilde{\nu}_{i} \nabla h_{i}(\tilde{\mathbf{x}})=\mathbf{0}
$$

\subsection*{KKT Conditions}
\begin{itemize}[leftmargin=*]
    \item primal and dual feasibilty
    \item complementary slackness
    \item vanishing Lagrangian gradient
\end{itemize}

$$
\begin{aligned}
f_{i}(\tilde{x}) & \leq 0, \quad i=1, \ldots, m \\
h_{i}(\tilde{x}) &=0, \quad i=1, \ldots, p \\
\tilde{\lambda}_{i} & \geq 0, \quad i=1, \ldots, m \\
\tilde{\lambda}_{i} f_{i}(\tilde{x}) &=0, \quad i=1, \ldots, m \\
\nabla f_{0}(\tilde{x})+\sum_{i=1}^{m} \tilde{\lambda}_{i} \nabla f_{i}(\tilde{x})+\sum_{i=1}^{p} \tilde{\nu}_{i} \nabla h_{i}(\tilde{x})&=0
\end{aligned}
$$




\subsubsection*{Thm 2.52}
Suppose that all $f_{i}$ and $h_{i}$ are differentiable, all $f_{i}$ are convex, all $h_{i}$ are affine.
Let $\tilde{\mathbf{x}}$ and $(\tilde{\boldsymbol{\lambda}}, \tilde{\boldsymbol{\nu}})$ be such that the KKT conditions hold.
Then $\tilde{\mathbf{x}}$ and $(\tilde{\boldsymbol{\lambda}}, \tilde{\boldsymbol{\nu}})$ have zero duality gap and hence are primal and dual optimal solutions.





